{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OpenAI Direct API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install openai\n",
    "import os\n",
    "#import openai\n",
    "from google import genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# setup api key\n",
    "os.environ['GOOGLE_API_KEY'] = 'AIzaSyDw6YNJSiSjpNwZVu5wjCsx8ywVjpYnziY'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenAI Api Call function\n",
    "\n",
    "def get_openai_response(prompt, model='gpt-3.5-turbo'):\n",
    "    messages = [{'role': 'user', 'content': prompt}]\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=model,\n",
    "        messages = messages,\n",
    "        temperature = 0,\n",
    "    )\n",
    "    \n",
    "    return response.choice[0].message['content']\n",
    "\n",
    "get_openai_response(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Google Gemini Direct API Call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "client  = genai.Client()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'gemini-2.0-flash-exp'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 = 2\\n'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_genai_response(prompt, model=model):\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt\n",
    "    )\n",
    "    \n",
    "    return response.text\n",
    "\n",
    "get_genai_response(\"What is 1+1?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# API Call using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using OpenAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install --upgrade langchain\n",
    "# pip install langchain langchain-google-genai\n",
    "\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "\n",
    "llm_model = 'gpt-3.5-turbo'\n",
    "chat = ChatOpenAI(temperature=0.0, model=llm_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "template_string = \"\"\"Translate the text \\\n",
    "that is delimited by triple backticks \\\n",
    "into a style that is {style}. \\\n",
    "text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "prompt_template.messages[0].prompt\n",
    "prompt_template.messages[0].prompt.input_variables\n",
    "\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "customer_messages = prompt_template.format_messages(style=customer_style,text=customer_email)\n",
    "\n",
    "print(type(customer_messages))\n",
    "print(type(customer_messages[0]))\n",
    "\n",
    "# Call the LLM to translate to the style of the customer message\n",
    "customer_response = chat(customer_messages)\n",
    "\n",
    "print(customer_response.content)\n",
    "\n",
    "service_reply = \"\"\"Hey there customer, \\\n",
    "the warranty does not cover \\\n",
    "cleaning expenses for your kitchen \\\n",
    "because it's your fault that \\\n",
    "you misused your blender \\\n",
    "by forgetting to put the lid on before \\\n",
    "starting the blender. \\\n",
    "Tough luck! See ya!\n",
    "\"\"\"\n",
    "\n",
    "service_style_pirate = \"\"\"\\\n",
    "a polite tone \\\n",
    "that speaks in English Pirate\\\n",
    "\"\"\"\n",
    "\n",
    "service_messages = prompt_template.format_messages(\n",
    "    style=service_style_pirate,\n",
    "    text=service_reply)\n",
    "\n",
    "print(service_messages[0].content)\n",
    "\n",
    "service_response = chat(service_messages)\n",
    "print(service_response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Gemini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_google_genai import ChatGoogleGenerativeAI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model: ChatGoogleGenerativeAI = ChatGoogleGenerativeAI(\n",
    "    model='gemini-2.0-flash-exp',\n",
    "    api_key=os.environ['GOOGLE_API_KEY'],\n",
    "    temperature=0.0\n",
    ")\n",
    "\n",
    "response = model.invoke('Tell me what is 1 + 1 ?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1 + 1 = 2\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Okay, I understand. I'm a bit frustrated because my blender lid came off and splattered smoothie all over my kitchen walls. To make things even more annoying, the warranty doesn't cover the cost of cleaning it up. I could really use some help with this right now.\\n\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template_string = \"\"\"\n",
    "Translate the text that is delimited by triple backticks into a style that is {style}. \\ text: ```{text}```\n",
    "\"\"\"\n",
    "\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "prompt_template = ChatPromptTemplate.from_template(template_string)\n",
    "\n",
    "customer_style = \"\"\"American English \\\n",
    "in a calm and respectful tone\n",
    "\"\"\"\n",
    "\n",
    "customer_email = \"\"\"\n",
    "Arrr, I be fuming that me blender lid \\\n",
    "flew off and splattered me kitchen walls \\\n",
    "with smoothie! And to make matters worse, \\\n",
    "the warranty don't cover the cost of \\\n",
    "cleaning up me kitchen. I need yer help \\\n",
    "right now, matey!\n",
    "\"\"\"\n",
    "\n",
    "customer_messages = prompt_template.format_messages(style=customer_style,text=customer_email)\n",
    "\n",
    "response = model.invoke(customer_messages)\n",
    "response.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
